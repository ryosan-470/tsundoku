# A Thorough Introduction to Distributed Systems

https://medium.com/free-code-camp/a-thorough-introduction-to-distributed-systems-3b91562c9b3c

# はじめに

世界における技術のますますの発展とともに分散システムもますます広がってきました。それらはコンピューターサイエンスの分野において複雑で巨大なものとなっています。

この記事では分散システムの基本的な方法について紹介し、システムの深い詳細を紹介するのではなく分散システムの異なるカテゴリに関して垣間見ることになります。

## 分散システムとはなにか

分散システムの最も単純な定義はエンドユーザーには1つのコンピュータのように見えるように連携して動作するコンピュータの集合です。
これらのマシンは状態を共有しており、同時に動作し、システム全体への影響を及ぼさず独立して障害を起こします。
私はあなたがそれを全てよりよく理解できるようにシステムを分散させる例を通して段階的に取り組むことを提案します。

![以前のようなスタック](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1553738105224_image.png)


データベースを使ってみましょう。従来のデータベースは情報を取得/挿入したいときはいつでも1つのマシンのファイルシステムに保存しているため、そのマシンと直接やり取りをします。
このデータベースシステムでの分散とは、同じときに複数のマシンでこのデータベースを動かすことになります。ユーザーは自分が選択したどのマシンともやりとりでき、単一のマシンと会話していないことを伝えることはできません。もしノード 1にレコードを挿入すると、ノード3はそのレコードを返すことができます。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1553738367079_image.png)

## どうして分散システムなのか

システムは常に必要に応じて分散されます。問題の真実は分散システムの管理は落とし穴と地雷でいっぱいな複雑なトピックであるということです。デプロイや維持、分散システムのデバッグは頭痛の種であり、どうしてそちらに行くのでしょうか?

分散システムが可能にすることは**水平方向へのスケール** です。1つのデータベース・サーバーがある例に戻ってみると、更に多くのトラフィックをさばくための唯一の方法はデータベースが動いているハードウェアをアップグレードすることだけでした。これを **垂直方向のスケーリング** と呼びます。

**水平方向のスケーリング** はハードウェアをアップグレードするということではなく単に更に多くのコンピューターを追加するということです。

![水平方向のスケーリングはある境界から格安 になる](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1554006205048_image.png)


特定のしきい値を超えた後の垂直方向のスケーリングよりも大幅に安価ですが、それが優先されるような主なケースではありません。

垂直方向のスケーリングはパフォーマンスを最新のハードウェアの性能まで引き上げることができるだけです。この性能は中程度から大きなワークロードを抱えるテクノロジー企業にとっては不十分であるということが証明されています。

水平方向のスケーリングにおける最善なことはスケールできる量に制限がないということです。つまりいつでもパフォーマンスが劣化した環境に他のマシンを追加することで無限の可能性を秘めた性能を引き出すことができます。

分散システムから得られる利点は簡単なスケーリングだけではありません。**フォールトトレランス** と**低遅延** も同様に重要です。


- **フォールトトレランス** 2つのデータセンターにまたがって10台のマシンのクラスターは本質的に単一のマシンよりも耐性があります。もし1つのデータセンターが焼失してもアプリケーションは稼働し続けられます。
- **低遅延** 世界を旅するネットワークパケットの時間は光の速度という制約が物理的に存在します。例えばニューヨークとシドニー間の光ファイバーケーブル内の往復時間 (RTT) は最も短くて160msです。分散システムは2つの街にノードをおき、近いノードからトラフィックを返すことができます。

しかし分散システムを動かすためには具体的に同じ時間で複数のコンピューターに動かし、問題が起きた場合に対処するソフトウェアが必要です。これは簡単ではありません。

# データベースをスケーリングする

Webアプリケーションが大変有名になったと想像してみてください。さらに私達のデータベースが扱うクエリが秒間で2倍になったと想像してみてください。アプリケーションはすぐさまパフォーマンスが低下し、ユーザーはそれに気づくでしょう。

では一緒にデータベースをスケールアップし高負荷に耐えられるようにしていきましょう。

典型的なWebアプリケーションでは新しい情報を挿入したり古い情報を修正するよりも情報を読み取ることのほうが多いです。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1554007958279_image.png)


読み取りパフォーマンスを向上する方法として**マスタースレーブレプリケーション** 戦略と呼ばれる方法があります。ここでは新しく2つのデータベースサーバーをつくりメインのものと同期させます。これら2つのインスタンスからのみ読み取るようにします。

情報を修正または挿入するときはいつでもマスターデータベースに接続しに行きます。それは順番に変更をスレーブに非同期的に通知しデータを保存します。

おめでとうございます。ついに3倍もの読み込みクエリを実行できるようになりました。これでよいでしょうか?

# 分散システムのカテゴリ
## 落とし穴

さて、データベースの**ACID** 保証のうち**C** で表される一貫性が抜け落ちています。
ご存知の通り、ここには新しいレコードをデータベースにインサートしてすぐにそれを取得する読み込みクエリを実行すると何も返ってこないという問題があります。

マスターからスレーブへの新しい情報の伝搬は瞬時には行われません。実際には古くなった情報を取得できる時間枠があります。そうでない場合は、データが伝搬されるのを同期的に待つ必要があるため書き込みパフォーマンスが低下します。

分散システムにはいくつかのトレードオフがあります。特にこの問題は適切に拡張したいときに対処しなければならない問題です。

**スケールアップを継続する**
スレーブデータベースを利用するという手法では、ある程度の範囲まで読み込みトラフィックを水平にスケールアップすることができます。これは大変素晴らしいのでしが、書き込みトラフィックにはどこかで上限があることになります。なぜなら書き込みは1台でしか対応できていないからです。

ここにはあまり選択肢がありません。書き込みトラフィックを処理することができないために書き込みトラフィックを複数のサーバーに分割する必要があります。

1つの方法は[マルチマスターレプリケーション戦略](https://en.wikipedia.org/wiki/Multi-master_replication)を取ることです。スレーブからは読み込むだけだったことの代わりに、複数のマスターノードを持ち、読み書きに対応させるようにします。ただ残念なことに[重複を作成してしまう](http://datacharmer.blogspot.bg/2013/03/multi-master-data-conflicts-part-1.html)という (例えば2つのレコードを同じIDでインサートする)複雑な問題が発生します。

もう一つの方法として**シャーディング** (または**パーティショニング**)とよばれる方法を見てみましょう。

サーバーを複数の小さなサーバー二分割してシャーディングしたものを**シャード** と呼びます。これらのシャードはすべて異なるレコードを持っています。なので何のレコードをどのシャードにするかというルールを作成します。データが一様に分散されるようにルールを作ることは大変重要です。

よくある手法としてレコードにおける情報の範囲によって定義するという方法があります。(例えばユーザー名がA~D)

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1554255020145_image.png)


このシャーディングキーは任意のカラムと等しくない読み込みが発生するため、慎重に選ぶ必要があります。(例えば多くの人々の名前はZよりもCから始まる) 多くのリクエストを他のシャードよりも受け取るシャードのことを **ホットスポット** とよびこれは避けなければなりません。一度分割したものを再度シャーディングしなおすことは信じられないほど大変コストがかかるもので、[FourSquareの11時間の障害](https://mashable.com/2010/10/05/foursquare-downtime-post-mortem/#qyp__Q9UDkqW)のように多くのダウンタイムを引き起こすことになります。

例を単純にするために私達のクライアント (Railsアプリ) が各レコードに対しどのデータベースを使用するか知っていると仮定しましょう。また分割には多くの戦略がありますが、今回は概要を説明するために簡単な例を使用します。

私達は今おおくの勝利を収めました。書き込みトラフィックはN倍に増え、Nはシャードの数です。これは事実上上限がないということになります。分割によってどれだけ細かいパーティショニングが得られるか想像してみてください。


## 落とし穴

どんなソフトウェアエンジニアリングでも少なからずトレードオフがあり、今回も例外ではありません。シャーディングは簡単には実現できないので、[本当に必要となるまで](https://www.percona.com/blog/2009/08/06/why-you-dont-want-to-shard/)避けることが最善です。
パーティション化されたキー以外でクエリを実行したところ、信じられないほど非効率になりました。(それらはすべてのシャードを通過する必要があります) SQLの `JOIN` クエリは最も悪く、複雑なもののため実際には利用できなくなります。


## decentralized vs distributed

更に述べる前に、2つの用語について区別をつけるようにしましょう。
この2つの単語の音はよく似ていて論理的には同じような意味と結論付けることができますが、それらの違いは技術的および政治的に大きな影響を与えます。

**decentalized** とは技術的な意味では分散されていますが、 分散化システム全体が1人のアクターによって所有されているわけではありません。誰も分散システムを所有していないため、これはもはや分散されているとは言えません。

これは私達が現在利用しているようなほとんどのシステムが**分散型集中システム** と考えることができます。
それについてはあなたが考えている以上に、それはあなたが参加者の何人かが悪意を持っているケースを処理する必要があるので、decentalized されたシステムを構築することは難しいのです。あなたが知っているような自信ですべてのノードを管理している通常の分散システムではこのようなケースはありません。


> 注記: この定義はすでに多くの人が議論しており、多くの人々を混乱させています。(peer to peer / federated) 。[初期の頃にはそれは異なるものだと定義されていました。](https://ethereum.stackexchange.com/a/7829)とにかく定義として私があなたに与えたものは、ブロックチェーンと暗号通貨がその用語を広めたため、今最も広く使われていると感じているものです。
![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1558928746015_image.png)

# 分散システムのカテゴリ

さてここからは多くの分散システムのカテゴリについてみていき、多くに知られている本番で利用されている大きなシステムをリスト化していきます。表示されているそのような数字の部分は大きくなっておりこれを読んでいる時点ではおそらく大幅に大きいことに気をつけてください。

## 分散データストア

分散データストアは幅広く利用されており、分散データベースとして知られています。ほとんどの分散データベースはNoSQLというリレーショナルを持たないデータベースでキーバリューセマンティクスの制約を持っています。これらは一貫性または可用性を犠牲にして信じられないほどのパフォーマンスとスケーラビリティを持っています。


> Appleでは2015年において、10PBに登るデータを75000のApache Cassandoraノードに保存していることが知られています

**CAP定理** の説明をしないで分散データストアの議論をすることはできません。


## CAP定理

[2002年に証明された方法では](https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/)CAP定理によると分散データストアは一貫性、可用性、分断耐性を同時に実現することはできません。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1554633473239_image.png)


簡単な定義は、

- **一貫性** 順番に読み書きすることが期待されている。(数段楽前のデータベースのレプリケーションに関する注意点を覚えていますか?)
- **可用性** システム全体が落ちることはない。死んでいないノードは常にレスポンスを返します。
- **分断耐性** システムは機能し続け、ネットワークの分割にもかかわらず一貫性/可用性を保証しています

現実には分断耐性はあえらゆるデータストアに与えられている必要があります。[この素晴らしい記事の1つで](https://codahale.com/you-cant-sacrifice-partition-tolerance/)述べられているように、分断耐性の許容度がなければ、一貫性と可用性を得ることはできません。

次のことを考えてみましょう。2つのノードがあり情報を受理して接続が切れたとき、両者が利用可能であり、同時に一貫性を持つためにはどうしたら良いでしょうか? 他のノードが何をしているか知る方法はなく、そのためオフラインになる(利用できなくなる)か、古い情報を扱う(矛盾する) 可能性があります。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1554633883205_image.png)


結局の所、**ネットワーク分割の元で**システムの整合性を強くするか、可用性を高めるかを選択する必要があります。

実例ではほとんどのアプリケーションで可用性を選択しています。というのも常に強い一貫性は必要になることがないためです。それでも100%の可用性を保証する必要があるという理由で必ずしもそのトレードオフが行われるわけではありませんが、強力な一貫性を達成するためにマシンの同期をとる必要がある場合は、ネットワーク遅延が問題になる場合があります。これらや多くの要因がアプリケーションは通常、高可用性を提供するソリューションを選択します。

このようなデータベースは最も弱い一貫性モデル、つまり最終的な一貫性 (強い一貫性 vs 最終的な一貫性) で解決されます。このモデルは特定のアイテムに新しい更新が行われない場合、そのアイテムへのすべてのアクセスが最新の更新値を返すことを保証します。

これらのシステムは(従来のデータベースシステムのACIDとは対照的に)**BASE** 属性を提供します。


- **B**asically **A**vailable: システムは常にレスポンスを返す
- **S**oft state: システムは入力がないときでも(結果整合性を持つため)時間の経過とともに変化する可能性がある
- **E**ventually consistency: 入力がないときにデータは遅かれ早かれすべてのノードに拡散され一貫性が保たれます。

このような可用性を持つ分散データベースの例には、Cassandra, Riak, Voldemort があります。

もちろん更に強い整合性をもつデータストアとして、HBase, Couchbase, Redis, Zookeeper などがあります。

このCAP定理はそれだけで複数の記事になります。

- [クライアントの振る舞いによりシステムのCAP属性を微調整する](http://www.goland.org/blockchain_and_cap/)
- https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html

**Cassandra**
上で述べたCassandraはCAPのAP属性を優先し、結果整合性を保ちながら分散するNoSQLデータベースです。Cassandraは高度に設定可能であるため、これは少し誤解を招く可能性があることを認めなければなりません。可用性を犠牲にして強力な一貫性を提供することもできますがこれは一般的な使い方ではありません。

Cassandraはconsistent hashingを用いてクラスターのどのノードがユーザーが渡したデータを管理しているかを決定します。**replication factor** を設定することで、基本的にデータをレプリケートするノード数を表します。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1558929994959_image.png)


読み込むときはこれらのノードからのみ読み出します。
Cassandraは大規模にスケール可能でさらにとてつもない書き込みスループットを提供します。

![1秒あたりの書き込み数を示すおそらくバイアスされた図、引用](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1558930054321_image.png)


Even though this diagram might be biased and it looks like it compares Cassandra to databases set to provide strong consistency (otherwise I can’t see why MongoDB would drop performance when upgraded from 4 to 8 nodes), this should still show what a properly set up Cassandra cluster is capable of.

とにかく、水平スケーリングと信じられないほどのスループットを可能にする分散システムのトレードオフではCassandraはACIDデータベースのいくつかの基本的な機能であるトランザクションがありません。

**合意 / コンセンサス**
分散システムでデータベーストランザクションを実装することはトリッキーになります。というのも各ノードで正しい行動 (abort or commit) を同意する必要があるためです。これは**コンセンサス** として知られており、分散システムにおける基本的な問題です。

参加しているプロセスとネットワークが完全に再確立されている場合、「トランザクションコミット」問題に必要な種類の合意に達することは簡単です。しかし現実のシステムではプロセスがクラッシュしたりネットワークが分断されたり失ったり、メッセージが重複して届くなどいくつかの障害が起こるという課題があります。

これには問題があります。信頼できないネットワーク上で限られた時間内に正しい合意を得るということを保証することは不可能であるということが[証明されています](http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/)。

> FLP Impossibility として知られている問題。耐障害性 のある分散合意の不可能性
![](https://image.slidesharecdn.com/20180702-lets-learn-the-limit-of-distributed-systems-180702141536/95/-35-638.jpg?cb=1530583301)

> https://www.slideshare.net/ShingoOmura/ss-103946354 のスライドが日本語だと詳しそう

しかし実際には信頼できないネットワーク上で高速に合意を達成するアルゴリズムが存在します。Cassandraは実際に分散合意としてのアルゴリズムであるPaxosを用いた[軽量なトランザクション](https://www.beyondthelines.net/databases/cassandra-lightweight-transactions/)を提供しています。

## 分散コンピューティング

分散コンピューティングとは、最近みているビックデータ処理の流入の鍵です。1つのコンピュータでは現実的に処理できないような巨大なタスク (例えば、1000億レコードを集計する) を多くの小さなタスクに分割し、それをそれぞれ単一のマシンで実行するというやり方です。巨大なタスクをたくさんの小さなタスクへ分割し、多くのマシンで並行に実行しデータを集計することで最初の問題を解くことができます。この手法は水平スケーリングに再び帰着します。つまり大きなタスクについては単純にたくさんのノードを計算に追加すればいいのです。

この分野での早い開拓者としてGoogleがいて、彼らの巨大なデータを処理する必要性からMapReduceというパラダイムを発明しました。[2004年にはそれを論文として](http://static.googleusercontent.com/media/research.google.com/en/us/archive/mapreduce-osdi04.pdf)公開し、後にApache Hadoop がそれを元にオープンソースコミュニティとして立ち上がりました。

**MapReduce**
MapReduce は簡単にデータをマッピングするということと意味のあるものへreduceするという2つのステップから成り立っています。

次の例を再び取り上げます。

Mediumでは、保管の目的で大量の情報を2次分散データベースに保存していました。2017年4月 (1年前) からの拍手の数を取得したくなりました。

この例は限りなくシンプルで明瞭ですが、数十億の拍手を解析する必要があると想像してみてください。明らかに1台のマシンにこのすべての情報を保管したくはないですし、1台のマシンだけで解析することもしたくないです。また本番データベースではなく優先度の低いオフライン作業用に特別に構築された「ウェアハウス」データベースに対してもクエリを実行することはできません。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1559006077172_image.png)


それぞれのMapジョブが可能な限り多くのデータを変換する別々のノードになっています。
Each job traverses all of the data in the given storage node and maps it to a simple tuple of the date and the number one. Then, three intermediary steps *(which nobody talks about)* are done — Shuffle, Sort and Partition. They basically further arrange the data and delete it to the appropriate reduce job. As we’re dealing with big data, we have each Reduce job separated to work on a single date only.

これはよいパダライムであり、驚くほど様々なことができるようになります。つまり複数のMapReduceジョブをチェインすることができるのです。

**最良の方法**
MapReduceは今ではレガシーであり、いくつかの問題を持っています。ジョブはバッチとして動いており、もしジョブがどこかで失敗すると、全体を再起動する必要があります。ある2時間のジョブが失敗すると全体のデータ処理パイプラインを遅くすることができ、それがピーク時間に起こることを望みません。

もう一つの問題として、結果を得るまで待たなければならないということがあります。実際の解析システム (ビックデータを持ち、このように分散コンピューティングを用いる) では、最新のクランチデータを可能な限り新鮮なうちに利用することが重要です。

そのためこれらの問題に対応する他のアーキテクチャが現れてきました。つまり Lambda アーキテクチャとKappa アーキテクチャ です。これらの利点を新しいツールに導入しています。(Kafka Streams , Apache Spark, Apache Storm, Apache Samza)

## 分散ファイルシステム

分散ファイルシステムは分散データストアとして考えられています。同じ概念を持っていて、 データは大量のクラスターにまたがって保存してあり、1台にあるように見えるようになっています。それらは通常、分散コンピューティングと密接に関係しています。

> [YahooはHDFSを42000ノード以上で動かしており、600PB以上のデータを保持しています。(2011)](https://www.slideshare.net/Hadoop_Summit/what-it-takes-to-run-hadoop-at-scale-yahoo-perspectives)

WikipediaにはCassandora Query Language (CQL) のようなカスタムAPIを通してではなく、分散ファイルシステムがローカルファイルのように同じインターフェースとセマンティクスを持ち、ファイルにアクセスできるという違いを定義しています。

**HDFS**
Hadoop Distributed File System (HDFS) はHadoopフレームワークで利用される分散コンピューティングにおける分散ファイルシステムです。幅広く採用されているので、多数のマシンにまたがって大きなファイル (サイズがGBからTB)を保管および複製するために使用されています。

そのアーキテクチャは、主に**ネームノード** と **データノード** から成り立っています。ネームノードはどのノードがどのファイルブロックを保持しているかといったようなクラスタに関するメタデータを保持する責務を持っています。それにより、システムの健康状態を追跡しファイルを複製してどこに保存するのが最も最良であるかをネットワークにおいて調整する振る舞いをします。データノードは単にファイルを保持し、ファイルを複製し新しいものを書き込むといいようなコマンドを実行します。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1559054351998_image.png)


当然のことながら、HDFSは計算ジョブにデータ認識を提供するためHadoopでの計算に最も適しています。その後、上記のジョブはデータを保持しているノード上で実行されます。これはデータの局所性を活用しており、計算を最適化しネットワーク上のトラフィック量を削減します。

**IPFS**
[Interplanetary File System (IPFS)](https://ipfs.io/) は分散ファイルシステム用の面白いピア・ツー・ピアプロトコル/ネットワークです。ブロックチェーン技術で活用されており、それは単一の所有者も障害点もない、完全に分散化 (decentalized) されたアーキテクチャになっています。

IPFS はDNS同様のネーミングシステムであるIPNSを持っており、ユーザーは簡単に情報にアクセスすることができます。それはGitが行っているような歴史的バージョニングを経由してファイルを保存しています。これによりすべてのファイルの以前の状態にアクセスすることができます。

これはまだ開発途上であり (執筆時点でv0.4) ますが、すでにあるプロジェクトがこのファイルシステム上に構築されています。([FileCoin](https://filecoin.io/))

## 分散メッセージング

メッセージングシステムはシステム全体の中でメッセージ/イベントを保存および伝搬するための中心的な場所を提供します。これを使用することでアプリケーションロジックを他のシステムと対話することから分離することができます。

> [Linkedin におけるKafkaクラスターは1日に1兆件のメッセージを処理し、1秒間に450万件のメッセージを処理しました。](https://engineering.linkedin.com/apache-kafka/how-we_re-improving-and-advancing-kafka-linkedin)
![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1559055635426_image.png)


簡単に言えばメッセージングプラットフォームは次のように機能します。

あるメッセージが潜在的にメッセージを作成する (これを **producer** とよぶ) アプリケーションからブロードキャストされ、プラットフォームヘ向かい、(**consumer** とよばれる) 潜在的に複数のアプリケーションから読み取られます。

もしいくつかの場所であるデータを保持する必要があるときは、メッセージングプラットフォームはそのメッセージを広めるために最もクリーンな方法です。

Consumer は情報をブローカーから引き出す (pullモデル) または、ブローカーに情報を直接consumerにプッシュ (push モデル) させることができます。

 人気の高いメッセージングプラットフォームがいくつかあります。
 
 **RabbitMQ** ルーティングルールやその他の簡単に設定可能な設定を介してメッセージの軌跡をきめ細やかに制御できるメッセージブローカ。スマートブローカとは多くのロジックを含み、通過するメッセージを厳密に追跡することができます。**CAP** における **AP** と **CP** の両者を設定で行うことができ、プッシュモデルを用いてconsumerに通知します。
 
 **Kafka** メッセージブローカ (およびすべてのプラットフォーム) はやや低レベルで、どのメッセージが読み取られたかを追跡することもできず複雑なルーティングロジックも使用できません。これにより驚くべきパフォーマンスを実現しています。私の意見ではオープンソースコミュニティにより活発に開発されConfluentチームによりサポートされる最も見通しのあるプロジェクトです。Kafkaは間違いなくトップテック企業で最も広く使われています。[Kafkaの紹介と利点についての詳細を私は書いています。](https://hackernoon.com/thorough-introduction-to-apache-kafka-6fbf2989bbc1)
 
 **Apache ActiveMQ** 最も古いもので2004年からのプロジェクトです。JMS APIを用いるため、Java EEアプリケーションと連携できます。ActiveMQ Artemis として書き直され、Kafkaと同等の優れたパフォーマンスを提供します。
 
 **Amazon SQS** AWSにより提供されるメッセージングサービスです。すでに存在するアプリケーションに容易に組み込みことができ、Kafkaのようなシステムは設定することが難しいということで有名なので、独自のインフラストラクチャを扱う必要性を排除することができます。Amazon はさらに似ている2つのサービスとしてSNSとMQを用意しており、後者はAmazonにより管理されるActiveMQを基本としています。

## 分散アプリケーション

1つのデータベースに接続された単一のロードバランサの背後に5つのRailsサーバーを配置する場合、それは分散アプリケーションと呼ぶことはできるのでしょうか? 改めて上の定義を思い出してみてください。


> 分散システムとは、エンドユーザにとっては1つのコンピュータのように見える、複数のコンピュータが連携して動くものです。これらのマシンは状態を共有し、並列して操作し、システム全体に影響を与えることなく独立して故障する可能性があります。

データベースを状態を共有していると数えるのなら、これは分散システムとして分類できると主張できるでしょう。しかしそれは誤りです。なぜなら「一緒に動く」という定義の部分が欠け落ちているからです。

システムが分散されているというのは、ノードがそれぞれ通信し、強調して行動をしていることを指します。

これゆえにピア・ツー・ピアネットワークでバックエンドが可動しているようなアプリケーションは分散アプリケーションと分類することができるでしょう。それにもかかわらずこれはすべて不要な分類であり、目的に叶うものではアリませんが物事をまとめることがどれだけ難しいかを示しています。


> [2014年4月のGame of Thrones のエピソードに対して193000ノードのBitTorrent Swarm](https://torrentfreak.com/game-of-thrones-sets-new-torrent-swarm-record-140415/)

**Erlang仮想マシン**
Erlangは並行性、分散、フォールトトレランスに対する素晴らしいセマンティクスをもつ関数型言語です。Erlang仮想マシンはそれ自身を分散Erlangアプリケーションとして扱います。

そのモデルはシステム組み込みのメッセージパッシングにより互いにやり取りが行え多くの隔離された軽量プロセスで可動しているものです。これは [**アクターモデル**](http://berb.github.io/diploma-thesis/original/054_actors.html) とよばれ Erlang OTP ライブラリは (JVM用のAkkaの方針に従い) 分散アクターフレームワークと捉えることができます。

このモデルでは、かなり単純な同時実行性を実現するのに役立ちます。プロセスはそれらを実行しているシステムの使用可能なコアに分散されています。これはネットワーク設定 (メッセージをドロップする機能とは別に) から見分けることは難しく、Erlang VMは同じデータセンターや更に別の大陸に存在するようなErlang VMと接続することができます。この仮想マシンの集まりは1つのアプリケーションを動かし、引き継ぐ (他のノードに可動がスケジューリングされる) ことで障害を扱います。

実際に、言語における分散レイヤーはフォールトトレランスを提供するために追加されました。単一のマシンで可動するソフトウェアはそのマシンが死亡したり、アプリケーションがオフラインになるというリスクが常にあります。多くのノードで可動しているソフトウェアは、アプリケーションがそれを念頭に構築されていることで、ハードウェア障害への対応が容易になります。

**BitTorrent**
BitTorrentはtorrentを用いて巨大なファイルをWeb経由で転送するプロトコルとして最も広く使われています。主な考えはメインサーバーを経由する必要なく異なるピア間でファイル転送を促進するというものです。

BitTorrentクライアントを用いると、ファイルをダウンロードするために多数のコンピュータに接続します。.torrent ファイルを開くと、**tracker** とよばれる調整役として振る舞うマシンに接続します。それはピア発見に役立ち、必要なファイルがあるネットワーク内のノードを表示します。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1559101787228_image.png)


ユーザーには **leecher** と **seeder** という2つの概念があります。leecher はファイルをダウンロードするユーザーで、seeder はファイルをアップロードするユーザーです。

ピア・ツー・ピアネットワークについて面白いことは、あなたが普通のユーザーとしてネットワークに参加して貢献する能力を持っているということです。

BitTorrentやその前駆者 (Gnutella, Napster) はファイルをボランティアでホストし、そのファイルがほしいユーザーへアップロードすることができます。BitTorrentがとても人気があるのは、ネットワークに貢献するためのインセンティブを提供することが最初のものだったからです。ユーザーがファイルをダウンロードするだけの**フリーライディング** は以前のファイル共有プロトコルの問題でした。

BitTorrentはseederに最高のダウンロードを提供する人にもっとアップロードさせることである程度フリーライディングを解決しました。これはファイルをダウンロードしているときにアップロードする動機づけになります。残念ながらダウンロードが終わったあとに、ネットワーク内で活動的な状態を維持することはできません。これは完全なファイルを持っているネットワーク内のseederが不足を引き起こし、プロトコルはそのようなユーザーに大きく依存しているため、プライベートトラッカーのような解決策が実現しました。プライベートトラッカーは分散ネットワークに参加するために(ときおり招待制の)コミュニティのメンバーになる必要があります。

この分野での進歩により、トラッカーレスなtorrentが発明されました。これはBitTorrentプロトコルをアップグレードしたもので、メタデータを収集しピアを見つけるための中央トラッカーに依存せず、新しいアルゴリズムを用いています。そのようなものの例としてKademlia (Mainline DHT) があり、他のピアからピアを見つけ出すための分散ハッシュテーブル (DHT) です。実際には各ユーザーがトラッカーの役割を果たします。


## 分散元帳 (Distributed Ledgers)

分断元帳は分散ネットワーク内のすべてのノードで複製、同期、及び共有される不変の追加専用データベースとして考えることができます。

> 2018年1月4日のピークではEthereumネットワークの1日の取引が130万件に達しました。

それらはEvent Sourcingパターンを利用してあなたはその歴史の中でいつでも元帳の状態を再構築することができます。

**ブロックチェーン**
ブロックチェーンは現在分散元帳として使われている基盤となる技術であり実際はその始まりとなりました。分散空間におけるこの最近の偉大な革新により、BitCoinという最初の本当に分散化された支払いプロトコルが生まれました。

ブロックチェーンはネットワークが生まれてからのすべてのトランザクションを順序化したリストで保持している分散元帳です。トランザクションはグループ化されブロックに保存されています。ブロックチェーン全体は本質的にブロックの線形リストです。(名前もその様になっている) 前述したブロックは重い計算によって作成され、暗号化され互いに紐付いています。

簡単に言うと、各ブロックには現在のブロックの内容 (Merkle木の形式) の特別なハッシュ (X個のゼロからはじまる) と前のブロックのハッシュが含まれています。このハッシュを生成するためには、ブルートフォースでのみしか求める方法がないため多くのCPUパワーが必要です。

![簡略化したブロックチェーン](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1559105721468_image.png)


**マイナー** はブルートフォースによってハッシュを算出しようとするノードのことです。マイナーは皆ランダムな文字列 (**nonce** と呼ばれる)を思いつくことができる人を求め互いに競争します。これはコンテンツと組み合わせることで前述のハッシュを生成します。一度誰かが正しいnonceを見つけると、それをネットワーク全体に広報します。各ノードでその文字列が確認されるとチェインに受け入れられます。

これはブロックチェーンを変更することは非常にコストが掛かり、改ざんされていないことを検証するには非常に簡単なシステムとなります。

ブロックの内容を変更すると別のハッシュが生成されるためコストが掛かります。後続の各ブロックがそれに依存しているということを忘れないでください。もし上の写真にある1つ目のブロックを変更したいと考えた場合、Merkle木の親を変更する必要があります。これにより順にハッシュが変わっていき、ブロック2以降のハッシュも変更されていきます。つまり、一つブロックが変更されるとすべてのブロックに関するnonceをブルートフォースで見つけていく必要があります。

ネットワークは常に最も長い有効なチェーンを信頼し複製します。システムを騙し最終的に長いチェーンを生成するにはすべてのノードで使用される合計CPUパワーの50%以上が必要になります。

ブロックチェーンは**突発的コンセンサス** のための分散型メカニズムであると言えるでしょう。コンセンサスの達成は明確ではありません。コンセンサスが発生するときに投票や瞬間的な時間は存在しません。代わりに合意はすべてがプロトコル規則に従い何千者独立したノードの非同期的な相互作用の突発的な成果なのです。

この今までにない革新は現在技術分野でブームになっており、それが[Web 3.0](https://medium.com/@matteozago/why-the-web-3-0-matters-and-you-should-know-about-it-a5851d63c949)の誕生を記念するものと予測しています。確実にソフトウェアエンジニアリングの業界でもっとも面白い領域になり、その問題を解くために極めて挑戦的で面白い問題が待っているでしょう。

**Bitcoin**
それまでの分散支払いプロトコルにおいて足りなかったものは、[2重払問題](https://en.wikipedia.org/wiki/Double-spending)をリアルタイムで防ぐ方法がなかったことでした。研究により興味深い命題が作り出されましたが、Bitcoinが最初に他の利点を超える実用的な解決策を実装しました。

2重支払いの問題とは1人の行為者 (例えばbob) が、自身の単一のリソースを2箇所で利用できないということです。もしBobが1ドル持っていると、AliceとZackの両者にあげることができません。それは一つの資産であり重複できないのです。分散システムにおいて真にこの保証を達成することは本当に難しいということがわかっています。ブロックチェーンより以前に[いくつか興味深い緩和策](https://arxiv.org/abs/0802.0832v1)がありますが、実際的な方法で問題を完全に解決するわけではありません。

2重支払いの問題はBitcoinにより容易に解決されます。なぜなら1つのブロックは1度のみチェインに追加されるためです。2重支払いの問題は単一のブロックでは起こりえず、もし同時に2つのブロックが作り出されたとしても、結果として最長のチェーンになるのは1つだけです。

![](https://paper-attachments.dropbox.com/s_7C5824095611B6F99CD897C257373A1B301D6BA090DAE1EE35920999DD48FC87_1559228434513_image.png)


BitcoinはCPUパワーを蓄積することの難しさに依存しています。

投票システムでは、攻撃者はネットワークにノードを追加するだけですみ(ネットワークへの無料アクセスが設計の目標であるためこれは簡単なことです) 、CPUパワーの部分で攻撃者は物理的な制限に直面します。ますます強力なハードウェアが必要になるためです。

また悪意のあるノードのグループが実際に攻撃を成功されるためには、ネットワークの演算能力の50%超を制御しなければなりません。それ以下では残りのネットワークが先に長いブロックチェーンを生成してしまいます。

**Ethereum**
Ethereumはプログラミング可能なブロックチェーンを基礎としたソフトウェアプラットフォームとして考えられています。独自の暗号通貨 (Ether) はブロックチェーンにおける**スマートコントラクト** の展開を促す燃料です。

スマートコントラクトは、Ethereumブロックチェーンにおける単一のトランザクションとして保存されるコード片です。コードを実行するために、スマートコントラクトを取引先として取引を発行するだけです。するとマイナーノードは順にコードを実行し、どんな変更が起こってもその取引は発生します。コードはEthereum仮想マシン内で実行されます。

スマートコントラクトを記述するために使われるEthereumのプログラミング言語として、*Solidity* があります。Solidityはチューリング完全なプログラミング言語であり、直接Ethereumブロックチェーンに接続できるため 残高や他のスマートコントラクトの結果といった状態を問い合わせることができます。無限ループを防ぐためコードを実行するためにEtherが必要になります。

ブロックチェーンは一連の状態変化としてとらえられる可能性があるため、Ethereumおよび類似のプラットフォーム上に多数の分散アプリケーション ([DApps](https://medium.com/the-mission/2018-the-year-of-dapps-dbe108860bcb)) が構築されています。

**分散元帳のさらなる用途**

- [存在証明](https://en.wikipedia.org/wiki/Proof_of_Existence): ある時点で特定のデジタル文書が存在したということの証明を匿名で安全に保管するサービス。文書の整合性やタイムスタンプ、所有権の確認に役立ちます。
- [Decentralized Autonomous Organizations (DAO)](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) : 組織の改善案の合意のためにブロックチェーンを利用する組織。例として[Dashの統治システム](https://www.dash.org/governance/)、[SmartCashプロジェクト](https://smartcash.cc/what-is-smartcash/)があります。
- Decentralized Authentication (分散型認証): ブロックチェーンにIDを保存してどこでもシングルサインオンを可能にする。Sovrin, Civic など。
# まとめ

この短い記事で、分散システムとはなにか、なぜそれを使うのかを定義しようとし、各カテゴリを大まかに見てきました。覚えておきたい大切なことは、

- 分散システムは複雑である
- 価格とスケール性から選択する
- 分散システムをうまく動かすことは難しい
- CAP理論 / 整合性と可用性はトレードオフである
- 分散システムには、データストア、コンピューティング、ファイルシステム、メッセージングシステム、元帳、アプリケーションの6つのカテゴリがある

率直に言うと、分散システムの表面には殆ど触れていません。合意、レプリケーション戦略、イベント順序と時間、フォールトトレランス (耐障害性)、ネットワークを利用したメッセージのブロードキャストなど、中核的な問題を徹底的に説明することはできませんでした。


# その他

先に日本語で訳されていました。(8割方訳してから気づいた 😭 )

- [分散型システム徹底入門: Part 1](https://postd.cc/a-thorough-introduction-to-distributed-systems-3/)
- [分散型システム徹底入門: Part 2](https://postd.cc/a-thorough-introduction-to-distributed-systems-2/)
- [分散型システム徹底入門: Part 3](https://postd.cc/a-thorough-introduction-to-distributed-systems/)


